# -*- coding: utf-8 -*-
"""DL PRoject 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17pH2_h1MpbS78d2xUp92Ggy9Rb4zjGMu

# Acquire the data

First, we read the data from google drive in order to perform the training.

The data already has data augmenttaion.

We use 20% of the data for validation
"""

# Create a generator based on the training data

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

training_gen = ImageDataGenerator(
    rescale=1./255,  # Use scale
    validation_split=0.2, # Use 20% for validation
    vertical_flip=True
    
) 

# Now the images are all 100x100

train_gen = training_gen.flow_from_directory(
    "drive/MyDrive/dl/train/", # Location of the training set in drive
    class_mode='categorical', # Use this for categories and clasification
    subset='training', # Tell keras this is the training set,
    target_size=(150,150)
)

val_gen = training_gen.flow_from_directory(
    "drive/MyDrive/dl/train/", # Location of the training set in drive
    class_mode='categorical', # Use this for categories and clasification
    subset='validation', # Tell keras this is the validation set
    target_size=(150,150)
)

"""# Defining the neural network

The network was designed by following multiple tutorials, including one fro tensorflow team at google themselves
"""

from keras import layers

model = keras.Sequential([
  # Conv section
  keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)),
  layers.MaxPooling2D(2,2),
  layers.Conv2D(64, (3,3), activation='relu'),
  layers.MaxPooling2D(2,2),
  layers.Conv2D(128, (3,3), activation='relu'),
  layers.MaxPooling2D(2,2),
  layers.Conv2D(128, (3,3), activation='relu'),
  layers.MaxPooling2D(2,2),

  # Flatten to normal NN
  layers.Flatten(),

  # Dropout cuz the indian guy says so
  layers.Dropout(0.5),

  # Hidden layer for the normal nn
  layers.Dense(512, activation='relu'),

  # Output layer with 3 classes: R P S
  layers.Dense(3, activation='softmax'),

])

"""# Training the network

To train the network using GPUs and TPUs (thanks google) we will firt compile the NN and then train it
"""

# COmpile the NN

model.compile(loss = 'categorical_crossentropy', 
              optimizer='rmsprop', 
              metrics=['accuracy']
          )

"""## Start the training using GPUs and TPUs"""

hist = model.fit_generator(train_gen, epochs=10, validation_data=val_gen, verbose=1)

"""This is the sumary of the trained model"""

# Plot what just happened

import matplotlib.pyplot as plt

# summarize history for accuracy

history = model.history

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""From this graph, we can conclude there is a bit of overfitting, even thought we attepmted to combat it via data ugmentation because we used vertical flip to double the images, but we can conclude it was not enogh and we would either need more data, or more augmentation, like brightness adjustments or partial rotations

# Using the trained model

Once the model has been trained with a more or less good validation accuracy (well not that good, thats a sign of overfitting), we can test it wit real data. You can get the image from the test folder to see what prediction it gets
"""

# Grab some images for testing

from keras.preprocessing import  image

import numpy as np


# PAPER: first one should light up
# ROCK: Middle should fire up

img = image.load_img('drive/MyDrive/dl/test/rock/testrock04-00.png', target_size=(150,150))

img_array = image.img_to_array(img)
img_batch = np.expand_dims(img_array, axis=0)

model.predict(img_batch, verbose=1)